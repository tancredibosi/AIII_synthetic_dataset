{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giovanni Grotto - giovanni.grotto@studio.unibo.it <br>\n",
    "Francesco Farneti - francesco.farneti7@studio.unibo.it <br>\n",
    "Tancredi Bosi - tancredi.bosi@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter\n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "# Suppress user warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Dataset_2.0_Akkodis.xlsx'\n",
    "original_data = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data(data):\n",
    "    \"\"\"\n",
    "    Cleans and organizes the dataset.\n",
    "    - Strips whitespace from column names and string values.\n",
    "    - Removes duplicate rows.\n",
    "    - Cleans the 'Overall' column and splits 'Residence' column into separate columns.\n",
    "    - Converts 'Year of insertion' and 'Year of Recruitment' columns to integers.\n",
    "    - Filters out rows with invalid 'Last Role' values.\n",
    "    - Groups rows with the same ID by keeping the one with the most non-NaN values.\n",
    "    - Drops unnecessary columns.\n",
    "    \"\"\"\n",
    "    data.columns = data.columns.str.strip()  # Remove leading/trailing spaces from column names\n",
    "    data = data.map(lambda x: x.strip() if isinstance(x, str) else x)  # Strip string values in all cells\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Drop the tilde in the 'Overall' column\n",
    "    data['Overall'] = data['Overall'].str.lstrip('~ ')\n",
    "\n",
    "    # Extract 'City', 'Province' and 'Region' from the column 'Residence'\n",
    "    data[['City', 'Province', 'Region']] = data['Residence'].str.split(' Â» | ~ ', expand=True)\n",
    "\n",
    "    # Convert the columns 'Year of insertion' and 'Year of Recruitment' to integers\n",
    "    data['Year of insertion'] = pd.to_numeric(data['Year of insertion'].str.strip('[]'), errors='coerce').astype(\n",
    "        'Int64')\n",
    "    data['Year of Recruitment'] = pd.to_numeric(data['Year of Recruitment'].str.strip('[]'), errors='coerce').astype(\n",
    "        'Int64')\n",
    "\n",
    "    # Remove invalid values in 'Last Role' column\n",
    "    undesired_values = ['????', '-', '.', '/']\n",
    "    data.loc[data['Last Role'].isin(undesired_values), 'Last Role'] = np.nan\n",
    "\n",
    "    # Group rows by ID, keeping the one with the most non-NaN values\n",
    "    def group_ids(df):\n",
    "        \"\"\"\n",
    "        Groups the dataset by ID, keeping the row with the highest number of non-NaN values for each ID.\n",
    "        \"\"\"\n",
    "        # Count non-NaN values in each row\n",
    "        df['non_nan_count'] = df.notna().sum(axis=1)\n",
    "        # Keep the row with the highest non-NaN count per ID\n",
    "        df = df.loc[df.groupby('ID')['non_nan_count'].idxmax()]\n",
    "        # Drop the helper column 'non_nan_count'\n",
    "        df = df.drop(columns=['non_nan_count'])\n",
    "        return df\n",
    "\n",
    "    data = group_ids(data)\n",
    "\n",
    "    # Drop columns that are not useful for the analysis\n",
    "    data = data.drop(columns=['linked_search__key', 'Years Experience.1', 'Study Area.1', 'Residence', 'Recruitment Request'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_minor_workers(data):\n",
    "    \"\"\"\n",
    "    Filters out workers who have inconsistencies in their age range and years of experience.\n",
    "    Specifically, removes rows with invalid combinations of 'Age Range' and 'Years Experience'.\n",
    "    \"\"\"\n",
    "    # Clean data from inconsistencies based on age and experience\n",
    "    invalid_mask = (\n",
    "            ((data['Age Range'] == '< 20 years') &\n",
    "             (data['Years Experience'].isin(['[+10]', '[7-10]', '[5-7]', '[3-5]']))) |\n",
    "            ((data['Age Range'] == '20 - 25 years') &\n",
    "             (data['Years Experience'] == '[+10]'))\n",
    "    )\n",
    "    # Remove invalid rows\n",
    "    return data[~invalid_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by removing special characters and converting it to lowercase.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    # Convert to lowercase\n",
    "    return text.lower()\n",
    "    \n",
    "def cluster_and_map_roles(unique_values):\n",
    "    \"\"\"\n",
    "    Clusters a list of unique values (e.g., job roles or tags) using hierarchical clustering.\n",
    "    Returns the cluster assignments and cluster names based on common words.\n",
    "    \"\"\"\n",
    "    # Preprocess the unique values (convert to lowercase and remove special characters)\n",
    "    processed_values = [preprocess_text(value) for value in unique_values]\n",
    "\n",
    "    # Create the TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(processed_values)\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='ward')\n",
    "    clusters = clustering.fit_predict(X.toarray())\n",
    "\n",
    "    # Create a dictionary that maps cluster IDs to the corresponding values\n",
    "    value_clusters = {}\n",
    "    for value, cluster_id in zip(unique_values, clusters):\n",
    "        if cluster_id not in value_clusters:\n",
    "            value_clusters[cluster_id] = []\n",
    "        value_clusters[cluster_id].append(value)\n",
    "\n",
    "    # Assign a name to each cluster based on the most common words in the values\n",
    "    cluster_names = {}\n",
    "    for cluster_id, values in value_clusters.items():\n",
    "        words = []\n",
    "        for value in values:\n",
    "            words.extend(preprocess_text(value).split())\n",
    "        common_words = [word for word, count in Counter(words).most_common(2)]\n",
    "        cluster_names[cluster_id] = \"-\".join(common_words) if common_words else \"Unknown\"\n",
    "\n",
    "    return clusters, cluster_names\n",
    "\n",
    "\n",
    "def map_to_cluster_name(value, unique_values, clusters, cluster_names):\n",
    "    \"\"\"\n",
    "    Maps a value to its corresponding cluster name.\n",
    "    \"\"\"\n",
    "    if value in unique_values:\n",
    "        # Find the cluster ID for the value\n",
    "        cluster_id = clusters[unique_values.index(value)]\n",
    "        # Return the corresponding cluster name, or the value itself if not found\n",
    "        return cluster_names.get(cluster_id, value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def cluster_tag(data):\n",
    "    \"\"\"\n",
    "    Applies clustering to 'Last Role' and 'TAG' columns in the dataset, assigning each value to a cluster name.\n",
    "    \"\"\"\n",
    "    # Extract unique values for 'Last Role' and 'TAG' columns\n",
    "    unique_last_roles = data['Last Role'].dropna().unique().tolist()\n",
    "    unique_tag = data['TAG'].dropna().unique().tolist()\n",
    "\n",
    "    # Perform clustering and assign names to clusters for 'Last Role' and 'TAG'\n",
    "    clusters_last_roles, cluster_names_last_roles = cluster_and_map_roles(unique_last_roles)\n",
    "    clusters_tags, cluster_names_tags = cluster_and_map_roles(unique_tag)\n",
    "\n",
    "    # Map the original 'Last Role' and 'TAG' values to their cluster names\n",
    "    data['Last Role'] = data['Last Role'].apply(\n",
    "        lambda role: map_to_cluster_name(role, unique_last_roles, clusters_last_roles, cluster_names_last_roles))\n",
    "    data['TAG'] = data['TAG'].apply(lambda tag: map_to_cluster_name(tag, unique_tag, clusters_tags, cluster_names_tags))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = organize_data(original_data)\n",
    "data = filter_minor_workers(data)\n",
    "data = cluster_tag(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check for inconsistencies in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(value, choose_second=False):\n",
    "    \"\"\"\n",
    "    Extracts the first or second number from a string.\n",
    "\n",
    "    Args:\n",
    "        value (str): The string containing numbers.\n",
    "        choose_second (bool): Flag to decide whether to return the second number (if any).\n",
    "\n",
    "    Returns:\n",
    "        int or None: The extracted number or None if no numbers are found.\n",
    "    \"\"\"\n",
    "    # Find all numbers in the string\n",
    "    matches = re.findall(r'\\d+', value)\n",
    "\n",
    "    # If the flag is set to choose the second number\n",
    "    if choose_second:\n",
    "        if len(matches) > 1:\n",
    "            return int(matches[1])  # Return the second number\n",
    "        elif len(matches) > 0:\n",
    "            return int(matches[0])  # Return the first if no second number\n",
    "    # If the flag is False, always return the first number if it exists\n",
    "    elif len(matches) > 0:\n",
    "        return int(matches[0])\n",
    "    return None  # Return None if no numbers are found\n",
    "    \n",
    "def check_constraint(row):\n",
    "    \"\"\"\n",
    "    Checks if a row violates any constraints.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row of data to check for constraint violations.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if any constraint violation is found, False otherwise.\n",
    "    \"\"\"\n",
    "    flag = False\n",
    "    flag += minor_worker_check(row)  # Check for minor worker constraint\n",
    "    flag += hired_check(row)  # Check for hired status constraint\n",
    "    return bool(flag)  # Return True if any constraint is violated\n",
    "\n",
    "\n",
    "def minor_worker_check(row):\n",
    "    \"\"\"\n",
    "    Checks if the worker was a minor when starting the job.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row of data containing age and work experience.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the worker was a minor at the time of hire, False otherwise.\n",
    "    \"\"\"\n",
    "    age = extract_number(row['Age Range'], choose_second=True)\n",
    "    work_exp = extract_number(row['Years Experience'])\n",
    "    if work_exp == 'nan':\n",
    "        return False\n",
    "    # The worker is a minor if their age at the start of work is less than 18\n",
    "    return (age - work_exp) < 18\n",
    "\n",
    "\n",
    "def hired_check(row):\n",
    "    \"\"\"\n",
    "    Checks if the 'Years of Recruitment' is less than or equal to 'Years of Insertion'\n",
    "    when the candidate's state is 'Hired'. If not hired, returns True for constraint violation.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row of data with candidate state and year information.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the constraint is violated, False otherwise.\n",
    "    \"\"\"\n",
    "    candidate_state = row['Candidate State']\n",
    "    years_insert = row['Year of insertion']\n",
    "    years_recruit = row['Year of Recruitment']\n",
    "\n",
    "    if candidate_state == 'Hired':\n",
    "        if pd.notna(years_insert) and pd.notna(years_recruit):\n",
    "            if years_insert > years_recruit:\n",
    "                return True  # Violation: Years of Insertion is greater than Recruitment\n",
    "    else:\n",
    "        if pd.notna(years_recruit):\n",
    "            return True  # Violation: 'Years of Recruitment' should be NaN if not hired\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mFound 0 inconsistencies in the original data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "inconsistencies_flag = data.apply(check_constraint, axis=1)\n",
    "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in the original data{Style.RESET_ALL}\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Display rows with violations\n",
    "violating_rows = data[inconsistencies_flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and set metadata for columns\n",
    "def get_metadata(data):\n",
    "    \"\"\"\n",
    "    Detects and sets metadata for the input data.\n",
    "    Specifically, it updates certain columns as categorical.\n",
    "    \"\"\"\n",
    "    # Detect metadata from the DataFrame\n",
    "    metadata = Metadata.detect_from_dataframe(data=data)\n",
    "\n",
    "    # List of columns to be marked as categorical\n",
    "    categorical_columns = ['Candidate State', 'Last Role', 'City', 'Province', 'Region']\n",
    "\n",
    "    # Update specified columns to be categorical\n",
    "    for column in categorical_columns:\n",
    "        metadata.update_column(column_name=column, sdtype='categorical')\n",
    "\n",
    "    # Validate the metadata\n",
    "    metadata.validate()\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata(data)\n",
    "synthesizer = GaussianCopulaSynthesizer(metadata, locales='it_IT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.auto_assign_transformers(data)\n",
    "synthesizer.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Synthetic Data without Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = synthesizer.sample(num_rows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconstistencies check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mFound 68 inconsistencies in the synthetic data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "inconsistencies_flag = synthetic_data.apply(check_constraint, axis=1)\n",
    "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in the synthetic data{Style.RESET_ALL}\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Display rows with violations\n",
    "violating_rows = synthetic_data[inconsistencies_flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Synthetic Data with Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and set constraints for the synthesizer\n",
    "def set_constraints(synthesizer):\n",
    "    \"\"\"\n",
    "    Loads custom constraints and adds them to the synthesizer.\n",
    "    \"\"\"\n",
    "    # Load the custom constraint classes\n",
    "    synthesizer.load_custom_constraint_classes(\n",
    "        filepath='custom_constraint_years.py',\n",
    "        class_names=['CustomYearsHired']\n",
    "    )\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = [\n",
    "        # Custom constraint for years hired\n",
    "        {\n",
    "            'constraint_class': 'CustomYearsHired',\n",
    "            'constraint_parameters': {'column_names': ['Candidate State', 'Year of insertion', 'Year of Recruitment']}\n",
    "        },\n",
    "        # Fixed combination constraints\n",
    "        {\n",
    "            'constraint_class': 'FixedCombinations',\n",
    "            'constraint_parameters': {'column_names': ['Candidate State', 'Year of Recruitment']}\n",
    "        },\n",
    "        {\n",
    "            'constraint_class': 'FixedCombinations',\n",
    "            'constraint_parameters': {'column_names': ['Age Range', 'Years Experience']}\n",
    "        },\n",
    "        {\n",
    "            'constraint_class': 'FixedCombinations',\n",
    "            'constraint_parameters': {'column_names': ['City', 'Province', 'Region']}\n",
    "        },\n",
    "        {\n",
    "            'constraint_class': 'FixedCombinations',\n",
    "            'constraint_parameters': {'column_names': ['event_type__val', 'event_feedback']}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Add constraints to the synthesizer\n",
    "    synthesizer.add_constraints(constraints=constraints)\n",
    "    return synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling rows: 100%|ââââââââââ| 1000/1000 [00:00<00:00, 5066.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mFound 0 inconsistencies in synthetic data with constraints\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Apply constraints to the synthesizer\n",
    "synthesizer = set_constraints(synthesizer)\n",
    "\n",
    "# Generate synthetic data with constraints\n",
    "synthesizer.fit(data)\n",
    "synthetic_data_with_constraints = synthesizer.sample(num_rows=1000)\n",
    "\n",
    "# Check for inconsistencies in the synthetic data with constraints\n",
    "inconsistencies_flag = synthetic_data_with_constraints.apply(check_constraint, axis=1)\n",
    "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in synthetic data with constraints{Style.RESET_ALL}\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Display rows with violations\n",
    "violating_rows = synthetic_data_with_constraints[inconsistencies_flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Polarized Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_matching_rows(df, exclusion_conditions):\n",
    "    \"\"\"\n",
    "    Removes rows from the DataFrame that match any set of conditions in exclusion_conditions.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        exclusion_conditions (list of lists): Each inner list contains dictionaries with 'Field' and 'Value' keys,\n",
    "                                             specifying conditions to exclude.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with matching rows removed.\n",
    "    \"\"\"\n",
    "    # Create an initial mask marking all rows as not removed\n",
    "    mask_to_remove = pd.Series(False, index=df.index)\n",
    "\n",
    "    for condition_group in exclusion_conditions:\n",
    "        # Start with all True mask (assume all rows match initially)\n",
    "        condition_mask = pd.Series(True, index=df.index)\n",
    "\n",
    "        for condition in condition_group:\n",
    "            field, value = condition[\"Field\"], condition[\"Value\"]\n",
    "            condition_mask &= (df[field] == value)\n",
    "\n",
    "        # Accumulate rows to remove using OR operation\n",
    "        mask_to_remove |= condition_mask\n",
    "\n",
    "    # Return DataFrame with unwanted rows removed\n",
    "    return df[~mask_to_remove]\n",
    "\n",
    "\n",
    "def filter_dataframe_by_constraints(df, constraints_list, total_rows):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on given constraints, ensuring the required number of rows per constraint.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        constraints_list (list of lists): Each inner list contains dictionaries with 'Field', 'Value', and 'Percentage' keys.\n",
    "        total_rows (int): Total number of rows to extract based on percentages.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame meeting all constraints.\n",
    "    \"\"\"\n",
    "    final_df = pd.DataFrame()  # Store the filtered results\n",
    "    used_indices = set()  # Track already used rows to avoid duplication\n",
    "\n",
    "    for i, constraint_group in enumerate(constraints_list):\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        # Exclude rows matching constraints from other groups\n",
    "        for j, other_group in enumerate(constraints_list):\n",
    "            if i != j:  # Skip current group\n",
    "                for constraint in other_group:\n",
    "                    field, value = constraint[\"Field\"], constraint[\"Value\"]\n",
    "                    temp_df = temp_df[temp_df[field] != value]\n",
    "\n",
    "        # Apply current group constraints\n",
    "        for constraint in constraint_group:\n",
    "            field, value = constraint[\"Field\"], constraint[\"Value\"]\n",
    "            num_required = (total_rows * constraint['Percentage']) // 100\n",
    "            temp_df = temp_df[temp_df[field] == value]\n",
    "\n",
    "        # Remove already used rows\n",
    "        temp_df = temp_df.loc[~temp_df.index.isin(used_indices)]\n",
    "\n",
    "        # Ensure the required number of elements are available\n",
    "        if len(temp_df) >= num_required:\n",
    "            temp_df = temp_df.head(num_required)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Not enough rows for constraints {constraint_group}, missing {num_required - len(temp_df)} elements\")\n",
    "\n",
    "        # Update used indices\n",
    "        used_indices.update(temp_df.index)\n",
    "\n",
    "        # Append to final DataFrame\n",
    "        final_df = pd.concat([final_df, temp_df])\n",
    "\n",
    "    return final_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000, scaling_factor=2,\n",
    "                                         max_retries=3):\n",
    "    \"\"\"\n",
    "    Generates synthetic data by applying a set of conditions from the polarization_list and retries if an error occurs.\n",
    "\n",
    "    Parameters:\n",
    "        synthesizer: The data synthesizer used to generate synthetic data.\n",
    "        polarization_list (list): List containing sets of conditions to apply to the generated data.\n",
    "        num_rows (int): The number of rows to generate (default is 1000).\n",
    "        scaling_factor (int): Factor to scale the number of rows on retries (default is 2).\n",
    "        max_retries (int): Maximum number of retries in case of errors (default is 3).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Generated synthetic data with the required constraints applied.\n",
    "    \"\"\"\n",
    "    synthetic_data_list = []\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            tot_rows = 0\n",
    "            for sublist in polarization_list:\n",
    "                n_elem = (num_rows * sublist[0]['Percentage']) // 100\n",
    "                tot_rows += n_elem\n",
    "                prev_percentage = None\n",
    "                col_values = {}\n",
    "                for el in sublist:\n",
    "                    if prev_percentage and prev_percentage != el['Percentage']:\n",
    "                        raise ValueError(\"Error: Mismatched percentages\")\n",
    "                    col_values[el['Field']] = el['Value']\n",
    "\n",
    "                condition = Condition(\n",
    "                    num_rows=n_elem * scaling_factor,\n",
    "                    column_values=col_values\n",
    "                )\n",
    "                polarized_synthetic_data = synthesizer.sample_from_conditions(\n",
    "                    conditions=[condition],\n",
    "                )\n",
    "                synthetic_data_list.append(polarized_synthetic_data)\n",
    "\n",
    "            # Combine all generated synthetic data\n",
    "            all_polarized_data = pd.concat(synthetic_data_list).reset_index(drop=True)\n",
    "            filtered_polarized_data = filter_dataframe_by_constraints(all_polarized_data, polarization_list, num_rows)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            scaling_factor *= 2  # Increase scaling factor on retry\n",
    "            print(\n",
    "                f\"{Fore.YELLOW}Retry {retries}/{max_retries}: Increasing scaling factor to {scaling_factor} due to error: {e}{Style.RESET_ALL}\")\n",
    "            time.sleep(1)\n",
    "            if retries == max_retries:\n",
    "                raise RuntimeError(\"Max retries reached. Unable to generate valid polarized synthetic data.\")\n",
    "\n",
    "    scaling_factor = 3\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            synthetic_data = synthesizer.sample(num_rows=num_rows * scaling_factor)\n",
    "            filtered_synthetic_data = exclude_matching_rows(synthetic_data, polarization_list)\n",
    "            fill_values = filtered_synthetic_data.sample(n=num_rows - tot_rows)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            scaling_factor *= 2  # Increase scaling factor on retry\n",
    "            print(\n",
    "                f\"{Fore.YELLOW}Retry {retries}/{max_retries}: Increasing scaling factor to {scaling_factor} due to error: {e}{Style.RESET_ALL}\")\n",
    "            time.sleep(1)\n",
    "            if retries == max_retries:\n",
    "                raise RuntimeError(\"Max retries reached. Unable to generate valid polarized synthetic data.\")\n",
    "\n",
    "    final_data = pd.concat([filtered_polarized_data, fill_values]).reset_index(drop=True)\n",
    "    return final_data\n",
    "\n",
    "def check_distribution_constraints(df, constraints_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution constraints are respected within the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to check.\n",
    "        constraints_list (list): A list of constraints (field-value pairs) to validate.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all distribution constraints are satisfied, False if any are violated.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    if total_rows == 0:\n",
    "        return False  # Return False if the dataframe is empty\n",
    "\n",
    "    for constraint_group in constraints_list:\n",
    "        filtered_df = df.copy()\n",
    "\n",
    "        # Filter the dataframe based on each condition in the constraint group\n",
    "        for condition in constraint_group:\n",
    "            field, value = condition[\"Field\"], condition[\"Value\"]\n",
    "            filtered_df = filtered_df[filtered_df[field] == value]\n",
    "\n",
    "        actual_count = len(filtered_df)\n",
    "        actual_percentage = (actual_count / total_rows) * 100\n",
    "        expected_percentage = constraint_group[0][\"Percentage\"]\n",
    "        expected_count = round((expected_percentage / 100) * total_rows)\n",
    "\n",
    "        # Compare the actual distribution with the expected distribution\n",
    "        if round(actual_percentage, 2) != round(expected_percentage, 2):\n",
    "            print(f\"{Fore.RED}Distribution of polarization not respected{Style.RESET_ALL}\")\n",
    "            print(f\"Condition: {constraint_group}\")\n",
    "            print(f\"Actual Count: {actual_count}, Expected Count: {expected_count}\")\n",
    "            print(f\"Actual Percentage: {actual_percentage}%, Expected Percentage: {expected_percentage}%\")\n",
    "            return False  # Constraint is not met\n",
    "\n",
    "    print(f\"{Fore.GREEN}Distribution of polarization respected{Style.RESET_ALL}\")\n",
    "    time.sleep(1)  # Add delay for better visualization of the result\n",
    "    return True  # All constraints are satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple polarization conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|ââââââââââ| 500/500 [00:00<00:00, 2163.57it/s]\n",
      "Sampling conditions: 100%|ââââââââââ| 500/500 [00:01<00:00, 363.49it/s]\n",
      "Sampling rows: 100%|ââââââââââ| 3000/3000 [00:00<00:00, 8459.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mDistribution of polarization respected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define polarization conditions\n",
    "polarization_list = [\n",
    "    [{\"Field\": \"Sex\", \"Value\": \"Female\", \"Percentage\": 25}],\n",
    "    [{\"Field\": \"Candidate State\", \"Value\": \"Hired\", \"Percentage\": 25}],\n",
    "]\n",
    "\n",
    "# Generate data with polarization\n",
    "final_data = polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000)\n",
    "\n",
    "# Check the distribution constraints of the polarized data\n",
    "check_distribution_constraints(final_data, polarization_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex polarization conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|ââââââââââ| 500/500 [00:01<00:00, 342.16it/s]\n",
      "Sampling conditions: 100%|ââââââââââ| 200/200 [00:00<00:00, 724.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mRetry 1/3: Increasing scaling factor to 4 due to error: Not enough rows for constraints [{'Field': 'Sex', 'Value': 'Female', 'Percentage': 25}, {'Field': 'Candidate State', 'Value': 'Hired', 'Percentage': 25}], missing 139 elements\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|ââââââââââ| 1000/1000 [00:02<00:00, 349.47it/s]\n",
      "Sampling conditions: 100%|ââââââââââ| 400/400 [00:00<00:00, 2034.32it/s]\n",
      "Sampling rows: 100%|ââââââââââ| 3000/3000 [00:00<00:00, 10796.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mDistribution of polarization respected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define another set of polarization conditions\n",
    "polarization_list = [\n",
    "    [{\"Field\": \"Sex\", \"Value\": \"Female\", \"Percentage\": 25},\n",
    "        {\"Field\": \"Candidate State\", \"Value\": \"Hired\", \"Percentage\": 25}],\n",
    "\n",
    "    [{\"Field\": \"Study Title\", \"Value\": \"Five-year degree\", \"Percentage\": 10},\n",
    "        {\"Field\": \"Assumption Headquarters\", \"Value\": \"Milan\", \"Percentage\": 10},\n",
    "        {\"Field\": \"English\", \"Value\": 3, \"Percentage\": 10}]\n",
    "]\n",
    "\n",
    "# Generate data with the second set of polarization conditions\n",
    "final_data = polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000)\n",
    "\n",
    "# Check the distribution constraints of the new polarized data\n",
    "check_distribution_constraints(final_data, polarization_list);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
