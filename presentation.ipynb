{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaYm58W2g73t"
      },
      "source": [
        "Giovanni Grotto - giovanni.grotto@studio.unibo.it <br>\n",
        "Francesco Farneti - francesco.farneti7@studio.unibo.it <br>\n",
        "Tancredi Bosi - tancredi.bosi@studio.unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyRZl5esg73z"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1jej7UehB1S",
        "outputId": "fe9b5373-eb6c-4456-fc7d-eb4160879131"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sdv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENAPvhCQhEwW",
        "outputId": "1f9f930f-4041-46c9-e049-5c96bb6e60b7"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sdv in /usr/local/lib/python3.11/dist-packages (1.19.0)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.28 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.37.28)\n",
            "Requirement already satisfied: botocore<2.0.0,>=1.31 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.37.28)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (3.1.1)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.11/dist-packages (from sdv) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.11/dist-packages (from sdv) (4.67.1)\n",
            "Requirement already satisfied: copulas>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.12.2)\n",
            "Requirement already satisfied: ctgan>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.11.0)\n",
            "Requirement already satisfied: deepecho>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.7.0)\n",
            "Requirement already satisfied: rdt>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.15.1)\n",
            "Requirement already satisfied: sdmetrics>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.19.0)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (4.3.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from sdv) (6.0.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2.0.0,>=1.28->sdv) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2.0.0,>=1.28->sdv) (0.11.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.3.0)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.11/dist-packages (from copulas>=0.12.1->sdv) (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from copulas>=0.12.1->sdv) (1.14.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ctgan>=0.11.0->sdv) (2.6.0+cu124)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->sdv) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->sdv) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->sdv) (1.6.1)\n",
            "Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->sdv) (37.1.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.31->sdv) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->sdv) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->sdv) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->ctgan>=0.11.0->sdv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->ctgan>=0.11.0->sdv) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "8sZ4F446g730"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from collections import Counter\n",
        "\n",
        "from colorama import Fore, Style\n",
        "\n",
        "from sdv.metadata import Metadata\n",
        "from sdv.single_table import GaussianCopulaSynthesizer\n",
        "from sdv.sampling import Condition\n",
        "\n",
        "# Suppress user warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdRe6NEyg731"
      },
      "source": [
        "## 2. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "8lnEbQkIg732"
      },
      "outputs": [],
      "source": [
        "file_path = 'Dataset_2.0_Akkodis.xlsx'\n",
        "original_data = pd.read_excel(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjluI84Ug732"
      },
      "source": [
        "## 3. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3_465wng732"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "O1qiFUFPg733"
      },
      "outputs": [],
      "source": [
        "def organize_data(data):\n",
        "    \"\"\"\n",
        "    Cleans and organizes the dataset.\n",
        "    - Strips whitespace from column names and string values.\n",
        "    - Removes duplicate rows.\n",
        "    - Cleans the 'Overall' column and splits 'Residence' column into separate columns.\n",
        "    - Converts 'Year of insertion' and 'Year of Recruitment' columns to integers.\n",
        "    - Filters out rows with invalid 'Last Role' values.\n",
        "    - Groups rows with the same ID by keeping the one with the most non-NaN values.\n",
        "    - Drops unnecessary columns.\n",
        "    \"\"\"\n",
        "    data.columns = data.columns.str.strip()  # Remove leading/trailing spaces from column names\n",
        "    data = data.map(lambda x: x.strip() if isinstance(x, str) else x)  # Strip string values in all cells\n",
        "\n",
        "    # Drop duplicate rows\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    # Drop the tilde in the 'Overall' column\n",
        "    data['Overall'] = data['Overall'].str.lstrip('~ ')\n",
        "\n",
        "    # Extract 'City', 'Province' and 'Region' from the column 'Residence'\n",
        "    data[['City', 'Province', 'Region']] = data['Residence'].str.split(' » | ~ ', expand=True)\n",
        "\n",
        "    # Clean the columns 'Year of insertion' and 'Year of Recruitment'\n",
        "    data['Year of insertion'] = data['Year of insertion'].str.strip('[]')\n",
        "    data['Year of Recruitment'] = data['Year of Recruitment'].str.strip('[]')\n",
        "\n",
        "    # Remove invalid values in 'Last Role' column\n",
        "    undesired_values = ['????', '-', '.', '/']\n",
        "    data.loc[data['Last Role'].isin(undesired_values), 'Last Role'] = np.nan\n",
        "\n",
        "    # Group rows by ID, keeping the one with the most non-NaN values\n",
        "    def group_ids(df):\n",
        "        \"\"\"\n",
        "        Groups the dataset by ID, keeping the row with the highest number of non-NaN values for each ID.\n",
        "        \"\"\"\n",
        "        # Count non-NaN values in each row\n",
        "        df['non_nan_count'] = df.notna().sum(axis=1)\n",
        "        # Keep the row with the highest non-NaN count per ID\n",
        "        df = df.loc[df.groupby('ID')['non_nan_count'].idxmax()]\n",
        "        # Drop the helper column 'non_nan_count'\n",
        "        df = df.drop(columns=['non_nan_count'])\n",
        "        return df\n",
        "\n",
        "    data = group_ids(data)\n",
        "\n",
        "    # Drop columns that are not useful for the analysis\n",
        "    data = data.drop(columns=['linked_search__key', 'Years Experience.1', 'Study Area.1', 'Residence', 'Recruitment Request'])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "hAEI_TP6g733"
      },
      "outputs": [],
      "source": [
        "def filter_minor_workers(data):\n",
        "    \"\"\"\n",
        "    Filters out workers who have inconsistencies in their age range and years of experience.\n",
        "    Specifically, removes rows with invalid combinations of 'Age Range' and 'Years Experience'.\n",
        "    \"\"\"\n",
        "    # Clean data from inconsistencies based on age and experience\n",
        "    invalid_mask = (\n",
        "            ((data['Age Range'] == '< 20 years') &\n",
        "             (data['Years Experience'].isin(['[+10]', '[7-10]', '[5-7]', '[3-5]']))) |\n",
        "            ((data['Age Range'] == '20 - 25 years') &\n",
        "             (data['Years Experience'] == '[+10]'))\n",
        "    )\n",
        "    # Remove invalid rows\n",
        "    return data[~invalid_mask].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "zUUAZOn6g734"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text by removing special characters and converting it to lowercase.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    # Convert to lowercase\n",
        "    return text.lower()\n",
        "\n",
        "def cluster_and_map_roles(unique_values):\n",
        "    \"\"\"\n",
        "    Clusters a list of unique values (e.g., job roles or tags) using hierarchical clustering.\n",
        "    Returns the cluster assignments and cluster names based on common words.\n",
        "    \"\"\"\n",
        "    # Preprocess the unique values (convert to lowercase and remove special characters)\n",
        "    processed_values = [preprocess_text(value) for value in unique_values]\n",
        "\n",
        "    # Create the TF-IDF matrix\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(processed_values)\n",
        "\n",
        "    # Perform hierarchical clustering\n",
        "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='ward')\n",
        "    clusters = clustering.fit_predict(X.toarray())\n",
        "\n",
        "    # Create a dictionary that maps cluster IDs to the corresponding values\n",
        "    value_clusters = {}\n",
        "    for value, cluster_id in zip(unique_values, clusters):\n",
        "        if cluster_id not in value_clusters:\n",
        "            value_clusters[cluster_id] = []\n",
        "        value_clusters[cluster_id].append(value)\n",
        "\n",
        "    # Assign a name to each cluster based on the most common words in the values\n",
        "    cluster_names = {}\n",
        "    for cluster_id, values in value_clusters.items():\n",
        "        words = []\n",
        "        for value in values:\n",
        "            words.extend(preprocess_text(value).split())\n",
        "        common_words = [word for word, count in Counter(words).most_common(2)]\n",
        "        cluster_names[cluster_id] = \"-\".join(common_words) if common_words else \"Unknown\"\n",
        "\n",
        "    return clusters, cluster_names\n",
        "\n",
        "\n",
        "def map_to_cluster_name(value, unique_values, clusters, cluster_names):\n",
        "    \"\"\"\n",
        "    Maps a value to its corresponding cluster name.\n",
        "    \"\"\"\n",
        "    if value in unique_values:\n",
        "        # Find the cluster ID for the value\n",
        "        cluster_id = clusters[unique_values.index(value)]\n",
        "        # Return the corresponding cluster name, or the value itself if not found\n",
        "        return cluster_names.get(cluster_id, value)\n",
        "    return value\n",
        "\n",
        "\n",
        "def cluster_tag(data):\n",
        "    \"\"\"\n",
        "    Applies clustering to 'Last Role' and 'TAG' columns in the dataset, assigning each value to a cluster name.\n",
        "    \"\"\"\n",
        "    # Extract unique values for 'Last Role' and 'TAG' columns\n",
        "    unique_last_roles = data['Last Role'].dropna().unique().tolist()\n",
        "    unique_tag = data['TAG'].dropna().unique().tolist()\n",
        "\n",
        "    # Perform clustering and assign names to clusters for 'Last Role' and 'TAG'\n",
        "    clusters_last_roles, cluster_names_last_roles = cluster_and_map_roles(unique_last_roles)\n",
        "    clusters_tags, cluster_names_tags = cluster_and_map_roles(unique_tag)\n",
        "\n",
        "    # Map the original 'Last Role' and 'TAG' values to their cluster names\n",
        "    data['Last Role'] = data['Last Role'].apply(\n",
        "        lambda role: map_to_cluster_name(role, unique_last_roles, clusters_last_roles, cluster_names_last_roles))\n",
        "    data['TAG'] = data['TAG'].apply(lambda tag: map_to_cluster_name(tag, unique_tag, clusters_tags, cluster_names_tags))\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd42hYA7g734"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "LngZB95Ag734"
      },
      "outputs": [],
      "source": [
        "data = organize_data(original_data)\n",
        "data = filter_minor_workers(data)\n",
        "data = cluster_tag(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rswY4Avpg735"
      },
      "source": [
        "## 4. Check for inconsistencies in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7YHXCyog735"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "rZVPl8zHg735"
      },
      "outputs": [],
      "source": [
        "def extract_number(value, choose_second=False):\n",
        "    \"\"\"\n",
        "    Extracts the first or second number from a string.\n",
        "\n",
        "    Args:\n",
        "        value (str): The string containing numbers.\n",
        "        choose_second (bool): Flag to decide whether to return the second number (if any).\n",
        "\n",
        "    Returns:\n",
        "        int or None: The extracted number or None if no numbers are found.\n",
        "    \"\"\"\n",
        "    # Find all numbers in the string\n",
        "    matches = re.findall(r'\\d+', value)\n",
        "\n",
        "    # If the flag is set to choose the second number\n",
        "    if choose_second:\n",
        "        if len(matches) > 1:\n",
        "            return int(matches[1])  # Return the second number\n",
        "        elif len(matches) > 0:\n",
        "            return int(matches[0])  # Return the first if no second number\n",
        "    # If the flag is False, always return the first number if it exists\n",
        "    elif len(matches) > 0:\n",
        "        return int(matches[0])\n",
        "    return None  # Return None if no numbers are found\n",
        "\n",
        "def check_constraint(row):\n",
        "    \"\"\"\n",
        "    Checks if a row violates any constraints.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row of data to check for constraint violations.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if any constraint violation is found, False otherwise.\n",
        "    \"\"\"\n",
        "    flag = False\n",
        "    flag += minor_worker_check(row)  # Check for minor worker constraint\n",
        "    flag += hired_check(row)  # Check for hired status constraint\n",
        "    return bool(flag)  # Return True if any constraint is violated\n",
        "\n",
        "\n",
        "def minor_worker_check(row):\n",
        "    \"\"\"\n",
        "    Checks if the worker was a minor when starting the job.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row of data containing age and work experience.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the worker was a minor at the time of hire, False otherwise.\n",
        "    \"\"\"\n",
        "    age = extract_number(row['Age Range'], choose_second=True)\n",
        "    work_exp = extract_number(row['Years Experience'])\n",
        "    if work_exp == 'nan':\n",
        "        return False\n",
        "    # The worker is a minor if their age at the start of work is less than 18\n",
        "    return (age - work_exp) < 18\n",
        "\n",
        "\n",
        "def hired_check(row):\n",
        "    \"\"\"\n",
        "    Checks if the 'Years of Recruitment' is less than or equal to 'Years of Insertion'\n",
        "    when the candidate's state is 'Hired'. If not hired, returns True for constraint violation.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row of data with candidate state and year information.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the constraint is violated, False otherwise.\n",
        "    \"\"\"\n",
        "    candidate_state = row['Candidate State']\n",
        "    years_insert = row['Year of insertion']\n",
        "    years_recruit = row['Year of Recruitment']\n",
        "\n",
        "    if candidate_state == 'Hired':\n",
        "        if pd.notna(years_insert) and pd.notna(years_recruit):\n",
        "            if years_insert > years_recruit:\n",
        "                return True  # Violation: Years of Insertion is greater than Recruitment\n",
        "    else:\n",
        "        if pd.notna(years_recruit):\n",
        "            return True  # Violation: 'Years of Recruitment' should be NaN if not hired\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f5rHEQDg735"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEoJEj-kg735",
        "outputId": "10940de3-75db-45f8-be65-ff92ef4a05fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mFound 0 inconsistencies in the original data\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "inconsistencies_flag = data.apply(check_constraint, axis=1)\n",
        "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in the original data{Style.RESET_ALL}\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Display rows with violations\n",
        "violating_rows = data[inconsistencies_flag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbxiQEKLg736"
      },
      "source": [
        "## 5. Synthesizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67tz71Dug736"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "3cw0cMXRg736"
      },
      "outputs": [],
      "source": [
        "# Function to detect and set metadata for columns\n",
        "def get_metadata(data):\n",
        "    \"\"\"\n",
        "    Detects and sets metadata for the input data.\n",
        "    Specifically, it updates certain columns as categorical.\n",
        "    \"\"\"\n",
        "    # Detect metadata from the DataFrame\n",
        "    metadata = Metadata.detect_from_dataframe(data=data)\n",
        "\n",
        "    # List of columns to be marked as categorical\n",
        "    categorical_columns = ['Candidate State', 'Last Role', 'City', 'Province', 'Region', 'Year of insertion', 'Year of Recruitment']\n",
        "\n",
        "    # Update specified columns to be categorical\n",
        "    for column in categorical_columns:\n",
        "        metadata.update_column(column_name=column, sdtype='categorical')\n",
        "\n",
        "    # Validate the metadata\n",
        "    metadata.validate()\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB_4o3U5g736"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "nNXvhNj7g736"
      },
      "outputs": [],
      "source": [
        "metadata = get_metadata(data)\n",
        "synthesizer = GaussianCopulaSynthesizer(metadata, locales='it_IT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Wu1QNwe2g736"
      },
      "outputs": [],
      "source": [
        "synthesizer.auto_assign_transformers(data)\n",
        "synthesizer.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NwMw0usg736"
      },
      "source": [
        "## 6. Synthetic Data without Constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "9KBZ9YiZg737"
      },
      "outputs": [],
      "source": [
        "synthetic_data = synthesizer.sample(num_rows=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7xJXf_Mg737"
      },
      "source": [
        "Inconstistencies check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyaZhC8ig737",
        "outputId": "ec06844f-90f5-4574-879d-5fdab5f81e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mFound 722 inconsistencies in the synthetic data\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "inconsistencies_flag = synthetic_data.apply(check_constraint, axis=1)\n",
        "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in the synthetic data{Style.RESET_ALL}\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Display rows with violations\n",
        "violating_rows = synthetic_data[inconsistencies_flag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzMjKBtJg737"
      },
      "source": [
        "## 7. Synthetic Data with Constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Yb_xjMg737"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "p7-jjk6Kg737"
      },
      "outputs": [],
      "source": [
        "# Function to load and set constraints for the synthesizer\n",
        "def set_constraints(synthesizer):\n",
        "    \"\"\"\n",
        "    Loads custom constraints and adds them to the synthesizer.\n",
        "    \"\"\"\n",
        "    # Load the custom constraint classes\n",
        "    synthesizer.load_custom_constraint_classes(\n",
        "        filepath='custom_constraint_years.py',\n",
        "        class_names=['CustomYearsHired']\n",
        "    )\n",
        "\n",
        "    # Define constraints\n",
        "    constraints = [\n",
        "        # Custom constraint for years hired\n",
        "        {\n",
        "            'constraint_class': 'CustomYearsHired',\n",
        "            'constraint_parameters': {'column_names': ['Candidate State', 'Year of insertion', 'Year of Recruitment']}\n",
        "        },\n",
        "        {\n",
        "            'constraint_class': 'FixedCombinations',\n",
        "            'constraint_parameters': {'column_names': ['Age Range', 'Years Experience']}\n",
        "        },\n",
        "        {\n",
        "            'constraint_class': 'FixedCombinations',\n",
        "            'constraint_parameters': {'column_names': ['City', 'Province', 'Region']}\n",
        "        },\n",
        "        {\n",
        "            'constraint_class': 'FixedCombinations',\n",
        "            'constraint_parameters': {'column_names': ['event_type__val', 'event_feedback']}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Add constraints to the synthesizer\n",
        "    synthesizer.add_constraints(constraints=constraints)\n",
        "    return synthesizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY8uSHNGg737"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nExGVfvWg737",
        "outputId": "1b8ce20f-ab7b-498a-8eb6-5020a5d0fdf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling rows: 100%|██████████| 10000/10000 [00:04<00:00, 2488.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mFound 0 inconsistencies in synthetic data with constraints\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Apply constraints to the synthesizer\n",
        "synthesizer = set_constraints(synthesizer)\n",
        "\n",
        "# Generate synthetic data with constraints\n",
        "synthesizer.fit(data)\n",
        "synthetic_data_with_constraints = synthesizer.sample(num_rows=10000)\n",
        "\n",
        "# Check for inconsistencies in the synthetic data with constraints\n",
        "inconsistencies_flag = synthetic_data_with_constraints.apply(check_constraint, axis=1)\n",
        "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in synthetic data with constraints{Style.RESET_ALL}\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Display rows with violations\n",
        "violating_rows = synthetic_data_with_constraints[inconsistencies_flag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzPLMgH6g737"
      },
      "source": [
        "## 8. Polarized Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3uARIgxg737"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "NiWQdqTog737"
      },
      "outputs": [],
      "source": [
        "def exclude_matching_rows(df, exclusion_conditions):\n",
        "    \"\"\"\n",
        "    Removes rows from the DataFrame that match any set of conditions in exclusion_conditions.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        exclusion_conditions (list of lists): Each inner list contains dictionaries with 'Field' and 'Value' keys,\n",
        "                                             specifying conditions to exclude.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Filtered DataFrame with matching rows removed.\n",
        "    \"\"\"\n",
        "    # Create an initial mask marking all rows as not removed\n",
        "    mask_to_remove = pd.Series(False, index=df.index)\n",
        "\n",
        "    for condition_group in exclusion_conditions:\n",
        "        # Start with all True mask (assume all rows match initially)\n",
        "        condition_mask = pd.Series(True, index=df.index)\n",
        "\n",
        "        for condition in condition_group:\n",
        "            field, value = condition[\"Field\"], condition[\"Value\"]\n",
        "            condition_mask &= (df[field] == value)\n",
        "\n",
        "        # Accumulate rows to remove using OR operation\n",
        "        mask_to_remove |= condition_mask\n",
        "\n",
        "    # Return DataFrame with unwanted rows removed\n",
        "    return df[~mask_to_remove]\n",
        "\n",
        "\n",
        "def filter_dataframe_by_constraints(df, constraints_list, total_rows):\n",
        "    \"\"\"\n",
        "    Filters the DataFrame based on given constraints, ensuring the required number of rows per constraint.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        constraints_list (list of lists): Each inner list contains dictionaries with 'Field', 'Value', and 'Percentage' keys.\n",
        "        total_rows (int): Total number of rows to extract based on percentages.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Filtered DataFrame meeting all constraints.\n",
        "    \"\"\"\n",
        "    final_df = pd.DataFrame()  # Store the filtered results\n",
        "    used_indices = set()  # Track already used rows to avoid duplication\n",
        "\n",
        "    for i, constraint_group in enumerate(constraints_list):\n",
        "        temp_df = df.copy()\n",
        "\n",
        "        # Exclude rows matching constraints from other groups\n",
        "        for j, other_group in enumerate(constraints_list):\n",
        "            if i != j:  # Skip current group\n",
        "                for constraint in other_group:\n",
        "                    field, value = constraint[\"Field\"], constraint[\"Value\"]\n",
        "                    temp_df = temp_df[temp_df[field] != value]\n",
        "\n",
        "        # Apply current group constraints\n",
        "        for constraint in constraint_group:\n",
        "            field, value = constraint[\"Field\"], constraint[\"Value\"]\n",
        "            num_required = (total_rows * constraint['Percentage']) // 100\n",
        "            temp_df = temp_df[temp_df[field] == value]\n",
        "\n",
        "        # Remove already used rows\n",
        "        temp_df = temp_df.loc[~temp_df.index.isin(used_indices)]\n",
        "\n",
        "        # Ensure the required number of elements are available\n",
        "        if len(temp_df) >= num_required:\n",
        "            temp_df = temp_df.head(num_required)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Not enough rows for constraints {constraint_group}, missing {num_required - len(temp_df)} elements\")\n",
        "\n",
        "        # Update used indices\n",
        "        used_indices.update(temp_df.index)\n",
        "\n",
        "        # Append to final DataFrame\n",
        "        final_df = pd.concat([final_df, temp_df])\n",
        "\n",
        "    return final_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000, scaling_factor=2,\n",
        "                                         max_retries=3):\n",
        "    \"\"\"\n",
        "    Generates synthetic data by applying a set of conditions from the polarization_list and retries if an error occurs.\n",
        "\n",
        "    Parameters:\n",
        "        synthesizer: The data synthesizer used to generate synthetic data.\n",
        "        polarization_list (list): List containing sets of conditions to apply to the generated data.\n",
        "        num_rows (int): The number of rows to generate (default is 1000).\n",
        "        scaling_factor (int): Factor to scale the number of rows on retries (default is 2).\n",
        "        max_retries (int): Maximum number of retries in case of errors (default is 3).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Generated synthetic data with the required constraints applied.\n",
        "    \"\"\"\n",
        "    synthetic_data_list = []\n",
        "\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            tot_rows = 0\n",
        "            for sublist in polarization_list:\n",
        "                n_elem = (num_rows * sublist[0]['Percentage']) // 100\n",
        "                tot_rows += n_elem\n",
        "                prev_percentage = None\n",
        "                col_values = {}\n",
        "                for el in sublist:\n",
        "                    if prev_percentage and prev_percentage != el['Percentage']:\n",
        "                        raise ValueError(\"Error: Mismatched percentages\")\n",
        "                    col_values[el['Field']] = el['Value']\n",
        "\n",
        "                condition = Condition(\n",
        "                    num_rows=n_elem * scaling_factor,\n",
        "                    column_values=col_values\n",
        "                )\n",
        "                polarized_synthetic_data = synthesizer.sample_from_conditions(\n",
        "                    conditions=[condition],\n",
        "                )\n",
        "                synthetic_data_list.append(polarized_synthetic_data)\n",
        "\n",
        "            # Combine all generated synthetic data\n",
        "            all_polarized_data = pd.concat(synthetic_data_list).reset_index(drop=True)\n",
        "            filtered_polarized_data = filter_dataframe_by_constraints(all_polarized_data, polarization_list, num_rows)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            scaling_factor *= 2  # Increase scaling factor on retry\n",
        "            print(\n",
        "                f\"{Fore.YELLOW}Retry {retries}/{max_retries}: Increasing scaling factor to {scaling_factor} due to error: {e}{Style.RESET_ALL}\")\n",
        "            time.sleep(1)\n",
        "            if retries == max_retries:\n",
        "                raise RuntimeError(\"Max retries reached. Unable to generate valid polarized synthetic data.\")\n",
        "\n",
        "    scaling_factor = 3\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            synthetic_data = synthesizer.sample(num_rows=num_rows * scaling_factor)\n",
        "            filtered_synthetic_data = exclude_matching_rows(synthetic_data, polarization_list)\n",
        "            fill_values = filtered_synthetic_data.sample(n=num_rows - tot_rows)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            scaling_factor *= 2  # Increase scaling factor on retry\n",
        "            print(\n",
        "                f\"{Fore.YELLOW}Retry {retries}/{max_retries}: Increasing scaling factor to {scaling_factor} due to error: {e}{Style.RESET_ALL}\")\n",
        "            time.sleep(1)\n",
        "            if retries == max_retries:\n",
        "                raise RuntimeError(\"Max retries reached. Unable to generate valid polarized synthetic data.\")\n",
        "\n",
        "    final_data = pd.concat([filtered_polarized_data, fill_values]).reset_index(drop=True)\n",
        "    return final_data\n",
        "\n",
        "def check_distribution_constraints(df, constraints_list):\n",
        "    \"\"\"\n",
        "    Checks if the distribution constraints are respected within the dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to check.\n",
        "        constraints_list (list): A list of constraints (field-value pairs) to validate.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all distribution constraints are satisfied, False if any are violated.\n",
        "    \"\"\"\n",
        "    total_rows = len(df)\n",
        "    if total_rows == 0:\n",
        "        return False  # Return False if the dataframe is empty\n",
        "\n",
        "    for constraint_group in constraints_list:\n",
        "        filtered_df = df.copy()\n",
        "\n",
        "        # Filter the dataframe based on each condition in the constraint group\n",
        "        for condition in constraint_group:\n",
        "            field, value = condition[\"Field\"], condition[\"Value\"]\n",
        "            filtered_df = filtered_df[filtered_df[field] == value]\n",
        "\n",
        "        actual_count = len(filtered_df)\n",
        "        actual_percentage = (actual_count / total_rows) * 100\n",
        "        expected_percentage = constraint_group[0][\"Percentage\"]\n",
        "        expected_count = round((expected_percentage / 100) * total_rows)\n",
        "\n",
        "        # Compare the actual distribution with the expected distribution\n",
        "        if round(actual_percentage, 2) != round(expected_percentage, 2):\n",
        "            print(f\"{Fore.RED}Distribution of polarization not respected{Style.RESET_ALL}\")\n",
        "            print(f\"Condition: {constraint_group}\")\n",
        "            print(f\"Actual Count: {actual_count}, Expected Count: {expected_count}\")\n",
        "            print(f\"Actual Percentage: {actual_percentage}%, Expected Percentage: {expected_percentage}%\")\n",
        "            return False  # Constraint is not met\n",
        "\n",
        "    print(f\"{Fore.GREEN}Distribution of polarization respected{Style.RESET_ALL}\")\n",
        "    time.sleep(1)  # Add delay for better visualization of the result\n",
        "    return True  # All constraints are satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeAEmld3g738"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIMMv-SUg738"
      },
      "source": [
        "Simple polarization conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPjF41akg738",
        "outputId": "ef17a86a-d7fb-498b-f326-f4e69d44ff3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling conditions: 100%|██████████| 500/500 [00:00<00:00, 595.70it/s] \n",
            "Sampling conditions: 100%|██████████| 500/500 [00:03<00:00, 126.61it/s]\n",
            "Sampling rows: 100%|██████████| 3000/3000 [00:01<00:00, 2102.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mDistribution of polarization respected\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Define polarization conditions\n",
        "polarization_list = [\n",
        "    [{\"Field\": \"Sex\", \"Value\": \"Female\", \"Percentage\": 25}],\n",
        "    [{\"Field\": \"Candidate State\", \"Value\": \"Hired\", \"Percentage\": 25}],\n",
        "]\n",
        "\n",
        "# Generate data with polarization\n",
        "final_data = polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000)\n",
        "\n",
        "# Check the distribution constraints of the polarized data\n",
        "check_distribution_constraints(final_data, polarization_list);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiETxoTtg738"
      },
      "source": [
        "Complex polarization conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGijPuMGg74B",
        "outputId": "4465439c-6b6f-4a79-cc81-28b989dc00ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling conditions: 100%|██████████| 500/500 [00:05<00:00, 96.21it/s] \n",
            "Sampling conditions: 100%|██████████| 200/200 [00:00<00:00, 382.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mRetry 1/3: Increasing scaling factor to 4 due to error: Not enough rows for constraints [{'Field': 'Sex', 'Value': 'Female', 'Percentage': 25}, {'Field': 'Candidate State', 'Value': 'Hired', 'Percentage': 25}], missing 141 elements\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling conditions: 100%|██████████| 1000/1000 [00:07<00:00, 141.78it/s]\n",
            "Sampling conditions: 100%|██████████| 400/400 [00:00<00:00, 702.13it/s]\n",
            "Sampling rows: 100%|██████████| 3000/3000 [00:01<00:00, 1513.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mDistribution of polarization respected\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Define another set of polarization conditions\n",
        "polarization_list = [\n",
        "    [{\"Field\": \"Sex\", \"Value\": \"Female\", \"Percentage\": 25},\n",
        "        {\"Field\": \"Candidate State\", \"Value\": \"Hired\", \"Percentage\": 25}],\n",
        "\n",
        "    [{\"Field\": \"Study Title\", \"Value\": \"Five-year degree\", \"Percentage\": 10},\n",
        "        {\"Field\": \"Assumption Headquarters\", \"Value\": \"Milan\", \"Percentage\": 10},\n",
        "        {\"Field\": \"English\", \"Value\": 3, \"Percentage\": 10}]\n",
        "]\n",
        "\n",
        "# Generate data with the second set of polarization conditions\n",
        "final_data = polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000)\n",
        "\n",
        "# Check the distribution constraints of the new polarized data\n",
        "check_distribution_constraints(final_data, polarization_list);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}