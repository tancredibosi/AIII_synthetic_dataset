{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp-G1I5Ve_Dk"
      },
      "source": [
        "Giovanni Grotto - giovanni.grotto@studio.unibo.it <br>\n",
        "Francesco Farneti - francesco.farneti7@studio.unibo.it <br>\n",
        "Tancredi Bosi - tancredi.bosi@studio.unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GRCpH1Ie_Dl"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxVli5JgfO8K",
        "outputId": "d33f98a0-fdd6-4403-a1ed-f97358c92040"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Ei9epufRhF",
        "outputId": "cbc9f71a-1238-4eec-aed0-5b86f2879272"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sdv\n",
            "  Downloading sdv-1.19.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting boto3<2.0.0,>=1.28 (from sdv)\n",
            "  Downloading boto3-1.37.28-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<2.0.0,>=1.31 (from sdv)\n",
            "  Downloading botocore-1.37.28-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (3.1.1)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.11/dist-packages (from sdv) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.11/dist-packages (from sdv) (4.67.1)\n",
            "Collecting copulas>=0.12.1 (from sdv)\n",
            "  Downloading copulas-0.12.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting ctgan>=0.11.0 (from sdv)\n",
            "  Downloading ctgan-0.11.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting deepecho>=0.7.0 (from sdv)\n",
            "  Downloading deepecho-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rdt>=1.14.0 (from sdv)\n",
            "  Downloading rdt-1.15.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sdmetrics>=0.19.0 (from sdv)\n",
            "  Downloading sdmetrics-0.19.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (4.3.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from sdv) (6.0.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.3.0)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.11/dist-packages (from copulas>=0.12.1->sdv) (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from copulas>=0.12.1->sdv) (1.14.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ctgan>=0.11.0->sdv) (2.6.0+cu124)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->sdv) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->sdv) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->sdv) (1.6.1)\n",
            "Collecting Faker>=17 (from rdt>=1.14.0->sdv)\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.31->sdv) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->sdv) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->sdv) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->ctgan>=0.11.0->sdv)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan>=0.11.0->sdv) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->ctgan>=0.11.0->sdv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->ctgan>=0.11.0->sdv) (3.0.2)\n",
            "Downloading sdv-1.19.0-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.37.28-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.28-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading copulas-0.12.2-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctgan-0.11.0-py3-none-any.whl (24 kB)\n",
            "Downloading deepecho-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading rdt-1.15.1-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sdmetrics-0.19.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, Faker, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, rdt, nvidia-cusolver-cu12, copulas, sdmetrics, boto3, deepecho, ctgan, sdv\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed Faker-37.1.0 boto3-1.37.28 botocore-1.37.28 copulas-0.12.2 ctgan-0.11.0 deepecho-0.7.0 jmespath-1.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rdt-1.15.1 s3transfer-0.11.4 sdmetrics-0.19.0 sdv-1.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jRI_h7Nre_Dl"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from collections import Counter\n",
        "\n",
        "from colorama import Fore, Style\n",
        "\n",
        "from sdv.metadata import Metadata\n",
        "from sdv.single_table import GaussianCopulaSynthesizer\n",
        "from sdv.sampling import Condition\n",
        "\n",
        "# Suppress user warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "728eTh6pe_Dm"
      },
      "source": [
        "## 2. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q42gJnbre_Dm"
      },
      "outputs": [],
      "source": [
        "file_path = 'Dataset_2.0_Akkodis.xlsx'\n",
        "original_data = pd.read_excel(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUTPwT5ke_Dm"
      },
      "source": [
        "## 3. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Zw_UMoe_Dm"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-4HAhUH_e_Dm"
      },
      "outputs": [],
      "source": [
        "def organize_data(data):\n",
        "    \"\"\"\n",
        "    Cleans and organizes the dataset.\n",
        "    - Strips whitespace from column names and string values.\n",
        "    - Removes duplicate rows.\n",
        "    - Cleans the 'Overall' column and splits 'Residence' column into separate columns.\n",
        "    - Converts 'Year of insertion' and 'Year of Recruitment' columns to integers.\n",
        "    - Filters out rows with invalid 'Last Role' values.\n",
        "    - Groups rows with the same ID by keeping the one with the most non-NaN values.\n",
        "    - Drops unnecessary columns.\n",
        "    \"\"\"\n",
        "    data.columns = data.columns.str.strip()  # Remove leading/trailing spaces from column names\n",
        "    data = data.map(lambda x: x.strip() if isinstance(x, str) else x)  # Strip string values in all cells\n",
        "\n",
        "    # Drop duplicate rows\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    # Drop the tilde in the 'Overall' column\n",
        "    data['Overall'] = data['Overall'].str.lstrip('~ ')\n",
        "\n",
        "    # Extract 'City', 'Province' and 'Region' from the column 'Residence'\n",
        "    data[['City', 'Province', 'Region']] = data['Residence'].str.split(' » | ~ ', expand=True)\n",
        "\n",
        "    # Convert the columns 'Year of insertion' and 'Year of Recruitment' to integers\n",
        "    data['Year of insertion'] = pd.to_numeric(data['Year of insertion'].str.strip('[]'), errors='coerce').astype(\n",
        "        'Int64')\n",
        "    data['Year of Recruitment'] = pd.to_numeric(data['Year of Recruitment'].str.strip('[]'), errors='coerce').astype(\n",
        "        'Int64')\n",
        "\n",
        "    # Remove invalid values in 'Last Role' column\n",
        "    undesired_values = ['????', '-', '.', '/']\n",
        "    data.loc[data['Last Role'].isin(undesired_values), 'Last Role'] = np.nan\n",
        "\n",
        "    # Group rows by ID, keeping the one with the most non-NaN values\n",
        "    def group_ids(df):\n",
        "        \"\"\"\n",
        "        Groups the dataset by ID, keeping the row with the highest number of non-NaN values for each ID.\n",
        "        \"\"\"\n",
        "        # Count non-NaN values in each row\n",
        "        df['non_nan_count'] = df.notna().sum(axis=1)\n",
        "        # Keep the row with the highest non-NaN count per ID\n",
        "        df = df.loc[df.groupby('ID')['non_nan_count'].idxmax()]\n",
        "        # Drop the helper column 'non_nan_count'\n",
        "        df = df.drop(columns=['non_nan_count'])\n",
        "        return df\n",
        "\n",
        "    data = group_ids(data)\n",
        "\n",
        "    # Drop columns that are not useful for the analysis\n",
        "    data = data.drop(columns=['linked_search__key', 'Years Experience.1', 'Study Area.1', 'Residence', 'Recruitment Request'])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5LHHEt9Me_Dn"
      },
      "outputs": [],
      "source": [
        "def filter_minor_workers(data):\n",
        "    \"\"\"\n",
        "    Filters out workers who have inconsistencies in their age range and years of experience.\n",
        "    Specifically, removes rows with invalid combinations of 'Age Range' and 'Years Experience'.\n",
        "    \"\"\"\n",
        "    # Clean data from inconsistencies based on age and experience\n",
        "    invalid_mask = (\n",
        "            ((data['Age Range'] == '< 20 years') &\n",
        "             (data['Years Experience'].isin(['[+10]', '[7-10]', '[5-7]', '[3-5]']))) |\n",
        "            ((data['Age Range'] == '20 - 25 years') &\n",
        "             (data['Years Experience'] == '[+10]'))\n",
        "    )\n",
        "    # Remove invalid rows\n",
        "    return data[~invalid_mask].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pC7W2UBBe_Dn"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text by removing special characters and converting it to lowercase.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    # Convert to lowercase\n",
        "    return text.lower()\n",
        "\n",
        "def cluster_and_map_roles(unique_values):\n",
        "    \"\"\"\n",
        "    Clusters a list of unique values (e.g., job roles or tags) using hierarchical clustering.\n",
        "    Returns the cluster assignments and cluster names based on common words.\n",
        "    \"\"\"\n",
        "    # Preprocess the unique values (convert to lowercase and remove special characters)\n",
        "    processed_values = [preprocess_text(value) for value in unique_values]\n",
        "\n",
        "    # Create the TF-IDF matrix\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(processed_values)\n",
        "\n",
        "    # Perform hierarchical clustering\n",
        "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='ward')\n",
        "    clusters = clustering.fit_predict(X.toarray())\n",
        "\n",
        "    # Create a dictionary that maps cluster IDs to the corresponding values\n",
        "    value_clusters = {}\n",
        "    for value, cluster_id in zip(unique_values, clusters):\n",
        "        if cluster_id not in value_clusters:\n",
        "            value_clusters[cluster_id] = []\n",
        "        value_clusters[cluster_id].append(value)\n",
        "\n",
        "    # Assign a name to each cluster based on the most common words in the values\n",
        "    cluster_names = {}\n",
        "    for cluster_id, values in value_clusters.items():\n",
        "        words = []\n",
        "        for value in values:\n",
        "            words.extend(preprocess_text(value).split())\n",
        "        common_words = [word for word, count in Counter(words).most_common(2)]\n",
        "        cluster_names[cluster_id] = \"-\".join(common_words) if common_words else \"Unknown\"\n",
        "\n",
        "    return clusters, cluster_names\n",
        "\n",
        "\n",
        "def map_to_cluster_name(value, unique_values, clusters, cluster_names):\n",
        "    \"\"\"\n",
        "    Maps a value to its corresponding cluster name.\n",
        "    \"\"\"\n",
        "    if value in unique_values:\n",
        "        # Find the cluster ID for the value\n",
        "        cluster_id = clusters[unique_values.index(value)]\n",
        "        # Return the corresponding cluster name, or the value itself if not found\n",
        "        return cluster_names.get(cluster_id, value)\n",
        "    return value\n",
        "\n",
        "\n",
        "def cluster_tag(data):\n",
        "    \"\"\"\n",
        "    Applies clustering to 'Last Role' and 'TAG' columns in the dataset, assigning each value to a cluster name.\n",
        "    \"\"\"\n",
        "    # Extract unique values for 'Last Role' and 'TAG' columns\n",
        "    unique_last_roles = data['Last Role'].dropna().unique().tolist()\n",
        "    unique_tag = data['TAG'].dropna().unique().tolist()\n",
        "\n",
        "    # Perform clustering and assign names to clusters for 'Last Role' and 'TAG'\n",
        "    clusters_last_roles, cluster_names_last_roles = cluster_and_map_roles(unique_last_roles)\n",
        "    clusters_tags, cluster_names_tags = cluster_and_map_roles(unique_tag)\n",
        "\n",
        "    # Map the original 'Last Role' and 'TAG' values to their cluster names\n",
        "    data['Last Role'] = data['Last Role'].apply(\n",
        "        lambda role: map_to_cluster_name(role, unique_last_roles, clusters_last_roles, cluster_names_last_roles))\n",
        "    data['TAG'] = data['TAG'].apply(lambda tag: map_to_cluster_name(tag, unique_tag, clusters_tags, cluster_names_tags))\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf-V6d3se_Do"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nbSr8zmFe_Do"
      },
      "outputs": [],
      "source": [
        "data = organize_data(original_data)\n",
        "data = filter_minor_workers(data)\n",
        "data = cluster_tag(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0PW5HBKe_Do"
      },
      "source": [
        "## 4. Check for inconsistencies in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCBr-8IVe_Do"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2BPbqoE9e_Do"
      },
      "outputs": [],
      "source": [
        "def extract_number(value, choose_second=False):\n",
        "    \"\"\"\n",
        "    Extracts the first or second number from a string.\n",
        "\n",
        "    Args:\n",
        "        value (str): The string containing numbers.\n",
        "        choose_second (bool): Flag to decide whether to return the second number (if any).\n",
        "\n",
        "    Returns:\n",
        "        int or None: The extracted number or None if no numbers are found.\n",
        "    \"\"\"\n",
        "    # Find all numbers in the string\n",
        "    matches = re.findall(r'\\d+', value)\n",
        "\n",
        "    # If the flag is set to choose the second number\n",
        "    if choose_second:\n",
        "        if len(matches) > 1:\n",
        "            return int(matches[1])  # Return the second number\n",
        "        elif len(matches) > 0:\n",
        "            return int(matches[0])  # Return the first if no second number\n",
        "    # If the flag is False, always return the first number if it exists\n",
        "    elif len(matches) > 0:\n",
        "        return int(matches[0])\n",
        "    return None  # Return None if no numbers are found\n",
        "\n",
        "def check_constraint(row):\n",
        "    \"\"\"\n",
        "    Checks if a row violates any constraints.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row of data to check for constraint violations.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if any constraint violation is found, False otherwise.\n",
        "    \"\"\"\n",
        "    flag = False\n",
        "    flag += minor_worker_check(row)  # Check for minor worker constraint\n",
        "    flag += hired_check(row)  # Check for hired status constraint\n",
        "    return bool(flag)  # Return True if any constraint is violated\n",
        "\n",
        "\n",
        "def minor_worker_check(row):\n",
        "    \"\"\"\n",
        "    Checks if the worker was a minor when starting the job.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row of data containing age and work experience.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the worker was a minor at the time of hire, False otherwise.\n",
        "    \"\"\"\n",
        "    age = extract_number(row['Age Range'], choose_second=True)\n",
        "    work_exp = extract_number(row['Years Experience'])\n",
        "    if work_exp == 'nan':\n",
        "        return False\n",
        "    # The worker is a minor if their age at the start of work is less than 18\n",
        "    return (age - work_exp) < 18\n",
        "\n",
        "\n",
        "def hired_check(row):\n",
        "    \"\"\"\n",
        "    Checks if the 'Years of Recruitment' is less than or equal to 'Years of Insertion'\n",
        "    when the candidate's state is 'Hired'. If not hired, returns True for constraint violation.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row of data with candidate state and year information.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the constraint is violated, False otherwise.\n",
        "    \"\"\"\n",
        "    candidate_state = row['Candidate State']\n",
        "    years_insert = row['Year of insertion']\n",
        "    years_recruit = row['Year of Recruitment']\n",
        "\n",
        "    if candidate_state == 'Hired':\n",
        "        if pd.notna(years_insert) and pd.notna(years_recruit):\n",
        "            if years_insert > years_recruit:\n",
        "                return True  # Violation: Years of Insertion is greater than Recruitment\n",
        "    else:\n",
        "        if pd.notna(years_recruit):\n",
        "            return True  # Violation: 'Years of Recruitment' should be NaN if not hired\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tCtGTM6e_Do"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG9HODYbe_Do",
        "outputId": "6a0afd62-f5ba-4b9d-8d86-2bf95e1bf0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mFound 0 inconsistencies in the original data\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "inconsistencies_flag = data.apply(check_constraint, axis=1)\n",
        "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in the original data{Style.RESET_ALL}\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Display rows with violations\n",
        "violating_rows = data[inconsistencies_flag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1iOfadQe_Dp"
      },
      "source": [
        "## 5. Synthesizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNZUNDpLe_Dp"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G25vz9hCe_Dp"
      },
      "outputs": [],
      "source": [
        "# Function to detect and set metadata for columns\n",
        "def get_metadata(data):\n",
        "    \"\"\"\n",
        "    Detects and sets metadata for the input data.\n",
        "    Specifically, it updates certain columns as categorical.\n",
        "    \"\"\"\n",
        "    # Detect metadata from the DataFrame\n",
        "    metadata = Metadata.detect_from_dataframe(data=data)\n",
        "\n",
        "    # List of columns to be marked as categorical\n",
        "    categorical_columns = ['Candidate State', 'Last Role', 'City', 'Province', 'Region']\n",
        "\n",
        "    # Update specified columns to be categorical\n",
        "    for column in categorical_columns:\n",
        "        metadata.update_column(column_name=column, sdtype='categorical')\n",
        "\n",
        "    # Validate the metadata\n",
        "    metadata.validate()\n",
        "    return metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk6oXwI6e_Dq"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VCE4cI75e_Dq"
      },
      "outputs": [],
      "source": [
        "metadata = get_metadata(data)\n",
        "synthesizer = GaussianCopulaSynthesizer(metadata, locales='it_IT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "56LFkqOBe_Dq"
      },
      "outputs": [],
      "source": [
        "synthesizer.auto_assign_transformers(data)\n",
        "synthesizer.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkF9R4Ose_Dq"
      },
      "source": [
        "## 6. Synthetic Data without Constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yNxAOiGe_Dq",
        "outputId": "ab67e1b9-cad0-49c1-a150-1204a7877844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling rows: 100%|██████████| 1000/1000 [00:04<00:00, 247.45it/s]\n"
          ]
        }
      ],
      "source": [
        "synthetic_data = synthesizer.sample(num_rows=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tISJ4S2Ie_Dq"
      },
      "source": [
        "Inconstistencies check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt3SwAaue_Dq",
        "outputId": "4f072d9f-9944-4664-ccb8-3e1bb1771d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mFound 68 inconsistencies in the synthetic data\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "inconsistencies_flag = synthetic_data.apply(check_constraint, axis=1)\n",
        "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in the synthetic data{Style.RESET_ALL}\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Display rows with violations\n",
        "violating_rows = synthetic_data[inconsistencies_flag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIhKAz9ie_Dq"
      },
      "source": [
        "## 7. Synthetic Data with Constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD3c7Vn1e_Dq"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YSkyelI8e_Dq"
      },
      "outputs": [],
      "source": [
        "# Function to load and set constraints for the synthesizer\n",
        "def set_constraints(synthesizer):\n",
        "    \"\"\"\n",
        "    Loads custom constraints and adds them to the synthesizer.\n",
        "    \"\"\"\n",
        "    # Load the custom constraint classes\n",
        "    synthesizer.load_custom_constraint_classes(\n",
        "        filepath='custom_constraint_years.py',\n",
        "        class_names=['CustomYearsHired']\n",
        "    )\n",
        "\n",
        "    # Define constraints\n",
        "    constraints = [\n",
        "        # Custom constraint for years hired\n",
        "        {\n",
        "            'constraint_class': 'CustomYearsHired',\n",
        "            'constraint_parameters': {'column_names': ['Candidate State', 'Year of insertion', 'Year of Recruitment']}\n",
        "        },\n",
        "        # Fixed combination constraints\n",
        "        {\n",
        "            'constraint_class': 'FixedCombinations',\n",
        "            'constraint_parameters': {'column_names': ['Candidate State', 'Year of Recruitment']}\n",
        "        },\n",
        "        {\n",
        "            'constraint_class': 'FixedCombinations',\n",
        "            'constraint_parameters': {'column_names': ['Age Range', 'Years Experience']}\n",
        "        },\n",
        "        {\n",
        "            'constraint_class': 'FixedCombinations',\n",
        "            'constraint_parameters': {'column_names': ['City', 'Province', 'Region']}\n",
        "        },\n",
        "        {\n",
        "            'constraint_class': 'FixedCombinations',\n",
        "            'constraint_parameters': {'column_names': ['event_type__val', 'event_feedback']}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Add constraints to the synthesizer\n",
        "    synthesizer.add_constraints(constraints=constraints)\n",
        "    return synthesizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cInFHm6le_Dr"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0EUpq8ye_Dr",
        "outputId": "86a1b4bf-7eb1-4e3c-e118-1ed30cc34496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling rows: 100%|██████████| 1000/1000 [00:00<00:00, 1192.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mFound 0 inconsistencies in synthetic data with constraints\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Apply constraints to the synthesizer\n",
        "synthesizer = set_constraints(synthesizer)\n",
        "\n",
        "# Generate synthetic data with constraints\n",
        "synthesizer.fit(data)\n",
        "synthetic_data_with_constraints = synthesizer.sample(num_rows=1000)\n",
        "\n",
        "# Check for inconsistencies in the synthetic data with constraints\n",
        "inconsistencies_flag = synthetic_data_with_constraints.apply(check_constraint, axis=1)\n",
        "print(f\"{Fore.GREEN}Found {inconsistencies_flag.sum()} inconsistencies in synthetic data with constraints{Style.RESET_ALL}\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Display rows with violations\n",
        "violating_rows = synthetic_data_with_constraints[inconsistencies_flag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoR-fMzhe_Dr"
      },
      "source": [
        "## 8. Polarized Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhPTQ5pze_Dr"
      },
      "source": [
        "##### FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4_5xch05e_Dr"
      },
      "outputs": [],
      "source": [
        "def exclude_matching_rows(df, exclusion_conditions):\n",
        "    \"\"\"\n",
        "    Removes rows from the DataFrame that match any set of conditions in exclusion_conditions.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        exclusion_conditions (list of lists): Each inner list contains dictionaries with 'Field' and 'Value' keys,\n",
        "                                             specifying conditions to exclude.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Filtered DataFrame with matching rows removed.\n",
        "    \"\"\"\n",
        "    # Create an initial mask marking all rows as not removed\n",
        "    mask_to_remove = pd.Series(False, index=df.index)\n",
        "\n",
        "    for condition_group in exclusion_conditions:\n",
        "        # Start with all True mask (assume all rows match initially)\n",
        "        condition_mask = pd.Series(True, index=df.index)\n",
        "\n",
        "        for condition in condition_group:\n",
        "            field, value = condition[\"Field\"], condition[\"Value\"]\n",
        "            condition_mask &= (df[field] == value)\n",
        "\n",
        "        # Accumulate rows to remove using OR operation\n",
        "        mask_to_remove |= condition_mask\n",
        "\n",
        "    # Return DataFrame with unwanted rows removed\n",
        "    return df[~mask_to_remove]\n",
        "\n",
        "\n",
        "def filter_dataframe_by_constraints(df, constraints_list, total_rows):\n",
        "    \"\"\"\n",
        "    Filters the DataFrame based on given constraints, ensuring the required number of rows per constraint.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        constraints_list (list of lists): Each inner list contains dictionaries with 'Field', 'Value', and 'Percentage' keys.\n",
        "        total_rows (int): Total number of rows to extract based on percentages.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Filtered DataFrame meeting all constraints.\n",
        "    \"\"\"\n",
        "    final_df = pd.DataFrame()  # Store the filtered results\n",
        "    used_indices = set()  # Track already used rows to avoid duplication\n",
        "\n",
        "    for i, constraint_group in enumerate(constraints_list):\n",
        "        temp_df = df.copy()\n",
        "\n",
        "        # Exclude rows matching constraints from other groups\n",
        "        for j, other_group in enumerate(constraints_list):\n",
        "            if i != j:  # Skip current group\n",
        "                for constraint in other_group:\n",
        "                    field, value = constraint[\"Field\"], constraint[\"Value\"]\n",
        "                    temp_df = temp_df[temp_df[field] != value]\n",
        "\n",
        "        # Apply current group constraints\n",
        "        for constraint in constraint_group:\n",
        "            field, value = constraint[\"Field\"], constraint[\"Value\"]\n",
        "            num_required = (total_rows * constraint['Percentage']) // 100\n",
        "            temp_df = temp_df[temp_df[field] == value]\n",
        "\n",
        "        # Remove already used rows\n",
        "        temp_df = temp_df.loc[~temp_df.index.isin(used_indices)]\n",
        "\n",
        "        # Ensure the required number of elements are available\n",
        "        if len(temp_df) >= num_required:\n",
        "            temp_df = temp_df.head(num_required)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Not enough rows for constraints {constraint_group}, missing {num_required - len(temp_df)} elements\")\n",
        "\n",
        "        # Update used indices\n",
        "        used_indices.update(temp_df.index)\n",
        "\n",
        "        # Append to final DataFrame\n",
        "        final_df = pd.concat([final_df, temp_df])\n",
        "\n",
        "    return final_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000, scaling_factor=2,\n",
        "                                         max_retries=3):\n",
        "    \"\"\"\n",
        "    Generates synthetic data by applying a set of conditions from the polarization_list and retries if an error occurs.\n",
        "\n",
        "    Parameters:\n",
        "        synthesizer: The data synthesizer used to generate synthetic data.\n",
        "        polarization_list (list): List containing sets of conditions to apply to the generated data.\n",
        "        num_rows (int): The number of rows to generate (default is 1000).\n",
        "        scaling_factor (int): Factor to scale the number of rows on retries (default is 2).\n",
        "        max_retries (int): Maximum number of retries in case of errors (default is 3).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Generated synthetic data with the required constraints applied.\n",
        "    \"\"\"\n",
        "    synthetic_data_list = []\n",
        "\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            tot_rows = 0\n",
        "            for sublist in polarization_list:\n",
        "                n_elem = (num_rows * sublist[0]['Percentage']) // 100\n",
        "                tot_rows += n_elem\n",
        "                prev_percentage = None\n",
        "                col_values = {}\n",
        "                for el in sublist:\n",
        "                    if prev_percentage and prev_percentage != el['Percentage']:\n",
        "                        raise ValueError(\"Error: Mismatched percentages\")\n",
        "                    col_values[el['Field']] = el['Value']\n",
        "\n",
        "                condition = Condition(\n",
        "                    num_rows=n_elem * scaling_factor,\n",
        "                    column_values=col_values\n",
        "                )\n",
        "                polarized_synthetic_data = synthesizer.sample_from_conditions(\n",
        "                    conditions=[condition],\n",
        "                )\n",
        "                synthetic_data_list.append(polarized_synthetic_data)\n",
        "\n",
        "            # Combine all generated synthetic data\n",
        "            all_polarized_data = pd.concat(synthetic_data_list).reset_index(drop=True)\n",
        "            filtered_polarized_data = filter_dataframe_by_constraints(all_polarized_data, polarization_list, num_rows)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            scaling_factor *= 2  # Increase scaling factor on retry\n",
        "            print(\n",
        "                f\"{Fore.YELLOW}Retry {retries}/{max_retries}: Increasing scaling factor to {scaling_factor} due to error: {e}{Style.RESET_ALL}\")\n",
        "            time.sleep(1)\n",
        "            if retries == max_retries:\n",
        "                raise RuntimeError(\"Max retries reached. Unable to generate valid polarized synthetic data.\")\n",
        "\n",
        "    scaling_factor = 3\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            synthetic_data = synthesizer.sample(num_rows=num_rows * scaling_factor)\n",
        "            filtered_synthetic_data = exclude_matching_rows(synthetic_data, polarization_list)\n",
        "            fill_values = filtered_synthetic_data.sample(n=num_rows - tot_rows)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            scaling_factor *= 2  # Increase scaling factor on retry\n",
        "            print(\n",
        "                f\"{Fore.YELLOW}Retry {retries}/{max_retries}: Increasing scaling factor to {scaling_factor} due to error: {e}{Style.RESET_ALL}\")\n",
        "            time.sleep(1)\n",
        "            if retries == max_retries:\n",
        "                raise RuntimeError(\"Max retries reached. Unable to generate valid polarized synthetic data.\")\n",
        "\n",
        "    final_data = pd.concat([filtered_polarized_data, fill_values]).reset_index(drop=True)\n",
        "    return final_data\n",
        "\n",
        "def check_distribution_constraints(df, constraints_list):\n",
        "    \"\"\"\n",
        "    Checks if the distribution constraints are respected within the dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to check.\n",
        "        constraints_list (list): A list of constraints (field-value pairs) to validate.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all distribution constraints are satisfied, False if any are violated.\n",
        "    \"\"\"\n",
        "    total_rows = len(df)\n",
        "    if total_rows == 0:\n",
        "        return False  # Return False if the dataframe is empty\n",
        "\n",
        "    for constraint_group in constraints_list:\n",
        "        filtered_df = df.copy()\n",
        "\n",
        "        # Filter the dataframe based on each condition in the constraint group\n",
        "        for condition in constraint_group:\n",
        "            field, value = condition[\"Field\"], condition[\"Value\"]\n",
        "            filtered_df = filtered_df[filtered_df[field] == value]\n",
        "\n",
        "        actual_count = len(filtered_df)\n",
        "        actual_percentage = (actual_count / total_rows) * 100\n",
        "        expected_percentage = constraint_group[0][\"Percentage\"]\n",
        "        expected_count = round((expected_percentage / 100) * total_rows)\n",
        "\n",
        "        # Compare the actual distribution with the expected distribution\n",
        "        if round(actual_percentage, 2) != round(expected_percentage, 2):\n",
        "            print(f\"{Fore.RED}Distribution of polarization not respected{Style.RESET_ALL}\")\n",
        "            print(f\"Condition: {constraint_group}\")\n",
        "            print(f\"Actual Count: {actual_count}, Expected Count: {expected_count}\")\n",
        "            print(f\"Actual Percentage: {actual_percentage}%, Expected Percentage: {expected_percentage}%\")\n",
        "            return False  # Constraint is not met\n",
        "\n",
        "    print(f\"{Fore.GREEN}Distribution of polarization respected{Style.RESET_ALL}\")\n",
        "    time.sleep(1)  # Add delay for better visualization of the result\n",
        "    return True  # All constraints are satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xlHHc-e_Ds"
      },
      "source": [
        "##### CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFAropm_e_Ds"
      },
      "source": [
        "Simple polarization conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1XOcF3le_Ds",
        "outputId": "4a71dd5d-0add-4dae-b4bc-1fc62a170d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling conditions: 100%|██████████| 500/500 [00:01<00:00, 494.36it/s] \n",
            "Sampling conditions: 100%|██████████| 500/500 [00:08<00:00, 59.79it/s]\n",
            "Sampling rows: 100%|██████████| 3000/3000 [00:01<00:00, 1635.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mDistribution of polarization respected\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Define polarization conditions\n",
        "polarization_list = [\n",
        "    [{\"Field\": \"Sex\", \"Value\": \"Female\", \"Percentage\": 25}],\n",
        "    [{\"Field\": \"Candidate State\", \"Value\": \"Hired\", \"Percentage\": 25}],\n",
        "]\n",
        "\n",
        "# Generate data with polarization\n",
        "final_data = polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000)\n",
        "\n",
        "# Check the distribution constraints of the polarized data\n",
        "check_distribution_constraints(final_data, polarization_list);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KebWaw6we_Dt"
      },
      "source": [
        "Complex polarization conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKWKWjUye_D2",
        "outputId": "0ae6972c-35f9-459e-9de0-d9a79c5f289e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling conditions: 100%|██████████| 500/500 [00:09<00:00, 52.26it/s]\n",
            "Sampling conditions: 100%|██████████| 200/200 [00:00<00:00, 249.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mRetry 1/3: Increasing scaling factor to 4 due to error: Not enough rows for constraints [{'Field': 'Sex', 'Value': 'Female', 'Percentage': 25}, {'Field': 'Candidate State', 'Value': 'Hired', 'Percentage': 25}], missing 135 elements\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling conditions: 100%|██████████| 1000/1000 [00:17<00:00, 57.23it/s]\n",
            "Sampling conditions: 100%|██████████| 400/400 [00:00<00:00, 528.96it/s]\n",
            "Sampling rows: 100%|██████████| 3000/3000 [00:01<00:00, 1900.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mDistribution of polarization respected\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Define another set of polarization conditions\n",
        "polarization_list = [\n",
        "    [{\"Field\": \"Sex\", \"Value\": \"Female\", \"Percentage\": 25},\n",
        "        {\"Field\": \"Candidate State\", \"Value\": \"Hired\", \"Percentage\": 25}],\n",
        "\n",
        "    [{\"Field\": \"Study Title\", \"Value\": \"Five-year degree\", \"Percentage\": 10},\n",
        "        {\"Field\": \"Assumption Headquarters\", \"Value\": \"Milan\", \"Percentage\": 10},\n",
        "        {\"Field\": \"English\", \"Value\": 3, \"Percentage\": 10}]\n",
        "]\n",
        "\n",
        "# Generate data with the second set of polarization conditions\n",
        "final_data = polarized_generation_from_conditions(synthesizer, polarization_list, num_rows=1000)\n",
        "\n",
        "# Check the distribution constraints of the new polarized data\n",
        "check_distribution_constraints(final_data, polarization_list);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}